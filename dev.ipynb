{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679462fa1ac37ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T04:50:32.552669Z",
     "start_time": "2024-08-17T04:50:31.707065Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import librosa\n",
    "# import pydub.silence\n",
    "# import pydub as pb\n",
    "# from joblib import Parallel, delayed\n",
    "# echonest = pd.read_csv(os.getcwd() + \"/data/echonest.csv\",low_memory=False)\n",
    "# tracks = pd.read_csv(os.getcwd() + \"/data/tracks.csv\",low_memory=False)\n",
    "\n",
    "# # <!-- - label using threshold for silence - maybe ~95%\n",
    "# # - use orignal data for labeling and serpeated data for labeling -->\n",
    "# labeling_data = os.getcwd() + \"/data/data\"\n",
    "# print(os.path.join(labeling_data,'000002.mp3'))\n",
    "# # try with audio with min silence\n",
    "# audio1  = pb.AudioSegment.from_mp3(os.path.join(labeling_data,'000005_bass.mp3'))\n",
    "# silent_ranges = pb.silence.detect_silence(audio1,silence_thresh=-40)        # silence  < thresh dbfs             (0 is max loud)\n",
    "# total_silence = sum((end - start) for start, end in silent_ranges)\n",
    "# total_duration = len(audio1)\n",
    "# print(total_silence / total_duration) # silence percent\n",
    "\n",
    "# # audio with mostly silence\n",
    "# audio  = pb.AudioSegment.from_mp3(os.path.join(labeling_data,'000140_drums.mp3'))\n",
    "# silent_ranges = pb.silence.detect_silence(audio,silence_thresh=-45)        # silence  < thresh dbfs             (0 is max loud)\n",
    "# total_silence = sum((end - start) for start, end in silent_ranges)\n",
    "# total_duration = len(audio)\n",
    "# (print(total_silence / total_duration)) # silence percent\n",
    "\n",
    "# def process_file(base_path, file, threshold):\n",
    "#     _aud = pb.AudioSegment.from_mp3(os.path.join(base_path, file))\n",
    "#     silence = pb.silence.detect_silence(_aud, silence_thresh=-45)\n",
    "#     silence = sum((e - s) for s, e in silence)\n",
    "#     percent = silence / len(_aud)\n",
    "#     _label = 1 if percent <= threshold else 0\n",
    "#     return [file, _label]\n",
    "\n",
    "# def lableler(base_path, threshold, num_files=None):\n",
    "#     files = os.listdir(base_path)[:num_files] if num_files else os.listdir(base_path)\n",
    "#     results = Parallel(n_jobs=-1)(delayed(process_file)(base_path, file, threshold) for file in files)\n",
    "#     _labels = pd.DataFrame(results, columns=['filename', 'label'])\n",
    "#     return _labels\n",
    "# # labels = lableler(labeling_data,0.8,50)\n",
    "# labels = lableler(labeling_data, threshold=0.8,num_files=1)\n",
    "# labels,audio1,audio,echonest,tracks\n",
    "\n",
    "# labels.to_csv(os.path.join(os.getcwd(),'data\\labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd794484b9560f7",
   "metadata": {},
   "source": [
    "instead of training a whole new model to detect these instruments <br>\n",
    "we can just use demucs to seperate and the above method to detect labels for instruments\n",
    "\n",
    "then seperate the others if possible\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e0ff51c03238cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:13:16.205632Z",
     "start_time": "2024-07-02T15:13:07.420981Z"
    }
   },
   "outputs": [],
   "source": [
    "# !demucs -d cuda \"data/data/000002.mp3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b01df46dbe43fb",
   "metadata": {},
   "source": [
    "seperate other instruments using unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e70eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import albumentations as A\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "# main_source https://github.com/kmaninis/OSVOS-PyTorch/blob/master/dataloaders/helpers.py\n",
    "import os.path\n",
    "import time\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms import CenterCrop, Pad\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb39af41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23464628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0):\n",
    "        super(CNNBlock, self).__init__()\n",
    "\n",
    "        self.seq_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq_block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f719767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlocks(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    n_conv (int): creates a block of n_conv convolutions\n",
    "    in_channels (int): number of in_channels of the first block's convolution\n",
    "    out_channels (int): number of out_channels of the first block's convolution\n",
    "    expand (bool) : if True after the first convolution of a blocl the number of channels doubles\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_conv,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 padding):\n",
    "        super(CNNBlocks, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_conv):\n",
    "\n",
    "            self.layers.append(CNNBlock(in_channels, out_channels, padding=padding))\n",
    "            # after each convolution we set (next) in_channel to (previous) out_channels\n",
    "            in_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d777c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    in_channels (int): number of in_channels of the first CNNBlocks\n",
    "    out_channels (int): number of out_channels of the first CNNBlocks\n",
    "    padding (int): padding applied in each convolution\n",
    "    downhill (int): number times a CNNBlocks + MaxPool2D it's applied.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 padding,\n",
    "                 downhill=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(downhill):\n",
    "            self.enc_layers += [\n",
    "                CNNBlocks(n_conv=2, in_channels=in_channels, out_channels=out_channels, padding=padding),\n",
    "                nn.MaxPool2d(2, 2)\n",
    "            ]\n",
    "\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        # doubling the dept of the last CNN block\n",
    "        self.enc_layers.append(CNNBlocks(n_conv=2, in_channels=in_channels,\n",
    "                                         out_channels=out_channels, padding=padding))\n",
    "\n",
    "    def forward(self, x):\n",
    "        route_connection = []\n",
    "        for layer in self.enc_layers:\n",
    "            if isinstance(layer, CNNBlocks):\n",
    "                x = layer(x)\n",
    "                route_connection.append(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x, route_connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911d9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    in_channels (int): number of in_channels of the first ConvTranspose2d\n",
    "    out_channels (int): number of out_channels of the first ConvTranspose2d\n",
    "    padding (int): padding applied in each convolution\n",
    "    uphill (int): number times a ConvTranspose2d + CNNBlocks it's applied.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 exit_channels,\n",
    "                 padding,\n",
    "                 uphill=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.exit_channels = exit_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(uphill):\n",
    "\n",
    "            self.layers += [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "                CNNBlocks(n_conv=2, in_channels=in_channels,\n",
    "                          out_channels=out_channels, padding=padding),\n",
    "            ]\n",
    "            in_channels //= 2\n",
    "            out_channels //= 2\n",
    "\n",
    "        # cannot be a CNNBlock because it has ReLU incorpored\n",
    "        # cannot append nn.Sigmoid here because you should be later using\n",
    "        # BCELoss () which will trigger the amp error \"are unsafe to autocast\".\n",
    "        self.layers.append(\n",
    "            nn.Conv2d(in_channels, exit_channels, kernel_size=1, padding=padding),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, routes_connection):\n",
    "        # pop the last element of the list since\n",
    "        # it's not used for concatenation\n",
    "        routes_connection.pop(-1)\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, CNNBlocks):\n",
    "                # center_cropping the route tensor to make width and height match\n",
    "                routes_connection[-1] = center_crop(routes_connection[-1], x.shape[2])\n",
    "                # concatenating tensors channel-wise\n",
    "                x = torch.cat([x, routes_connection.pop(-1)], dim=1)\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db21e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out unet shape is torch.Size([3, 1, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 first_out_channels,\n",
    "                 exit_channels,\n",
    "                 downhill,\n",
    "                 padding=0\n",
    "                 ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, first_out_channels, padding=padding, downhill=downhill)\n",
    "        self.decoder = Decoder(first_out_channels*(2**downhill), first_out_channels*(2**(downhill-1)),\n",
    "                               exit_channels, padding=padding, uphill=downhill)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out, routes = self.encoder(x)\n",
    "        out = self.decoder(enc_out, routes)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   #test\n",
    "\n",
    "    unet = UNET(1, 64, 1, padding=0, downhill=4)\n",
    "    test = torch.rand((3, 1, 572, 572))\n",
    "    print(\"out unet shape is\", unet(test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c833a",
   "metadata": {},
   "source": [
    "Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 388\n",
    "IMAGE_WIDTH = 388\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ROOT_DIR = os.getcwd() +\"/data/others/spectrograms\"\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "PAD_MIRRORING = 92\n",
    "SAVE_MODEL_PATH = os.getcwd() + \"/output/models\"\n",
    "SAVE_IMAGES_PATH =os.getcwd() + \"/output/segmented_images\"\n",
    "CHECKPOINT = os.getcwd() + \"/checkpoints\"\n",
    "CHECKPOINT = None    # remove after chekpointing\n",
    "\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        # add  more that do not alter audio signals\n",
    "        A.Normalize(\n",
    "            mean=(0, 0, 0),\n",
    "            std=(1, 1, 1),\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=(0, 0, 0),\n",
    "            std=(1, 1, 1),\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudDataSet(Dataset):\n",
    "    def __init__(self, directory, transform=None, precomputed_spectrograms=True):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.precomputed_spectrograms = precomputed_spectrograms\n",
    "        self.files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.directory, self.files[idx])\n",
    "        \n",
    "        if self.precomputed_spectrograms:\n",
    "        # Load the spectrogram\n",
    "            spectrogram = torch.load(file_path)\n",
    "        else:\n",
    "                # Load the raw audio\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "                \n",
    "            if self.transform:\n",
    "                spectrogram = self.transform(waveform)\n",
    "            else:\n",
    "                    # Default to MelSpectrogram if no transform is provided\n",
    "                spectrogram = MelSpectrogram()(waveform)\n",
    "            \n",
    "        return spectrogram.squeeze(0)  # Remove the channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751affb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a torch.Tensor to np.array, removes the batch_dim, if array has more than one channel\n",
    "# it transposes it from (CHANNELS, HEIGHT, WIDTH) to (HEIGHT, WIDTH, CHANNELS)\n",
    "def tens2image(im):\n",
    "    im = im.cpu()\n",
    "    # removing batch size\n",
    "    tmp = np.squeeze(im.numpy())\n",
    "    # if greyscale\n",
    "    if tmp.ndim == 2:\n",
    "        return tmp\n",
    "    else:\n",
    "        # in order to perform np.subtract and to plot images, we have to transfer channel to the last dimension\n",
    "        return tmp.transpose((1, 2, 0))\n",
    "\n",
    "def overlay_mask(im, ma, color=np.array([255, 0, 0]) / 255.0):\n",
    "    assert np.max(im) <= 255, \"RGB channels' value cannot exceed 255\"\n",
    "\n",
    "    # float to bool means: each not-0 is set to 1, each 0 is kept 0\n",
    "    ma = (ma > 0.5).astype(np.float32)\n",
    "    im = im.astype(np.uint8)\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # fg = im*alpha + np.ones(im.shape)*(1-alpha) * np.array([23,23,197])/255.0\n",
    "    # im (0, 1)\n",
    "    # color (0, 1)\n",
    "    # here we are creating a red transparent filter for the whole image that later will be\n",
    "    # used only in the perimeter of the masks\n",
    "    fg = im * alpha + np.ones(im.shape) * 255 * (1 - alpha) * color  # np.array([0,0,255])/255.0\n",
    "    # Whiten background\n",
    "    alpha = 1\n",
    "    bg = im.copy()\n",
    "\n",
    "    # create a simil_fg where fg is set to 0 where the mask is 0\n",
    "    # substitute the values of the gt_image where mask != 0 with the values of the simil_fg where values are != 0\n",
    "\n",
    "    # ma == 0 filters all the contour pixels of the mask\n",
    "    # + np.ones(im[ma == 0].shape) * (1 - alpha) if alpha < 1 is used to whiten the background\n",
    "    # if alpha=1 (default), + np.ones(im[ma == 0].shape) * (1 - alpha) is dropped\n",
    "\n",
    "    bg[ma == 0] = im[ma == 0] * alpha + np.ones(im[ma == 0].shape) * 255 * (1 - alpha)\n",
    "\n",
    "    bg[ma == 1] = fg[ma == 1]\n",
    "\n",
    "    # cv2.findContours(image, mode, method)\n",
    "    contours, _ = cv2.findContours(ma.copy().astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # cv2.drawContours(image, contours, colourIdx, color, thickness)\n",
    "    cv2.drawContours(bg, contours, -1, (0, 0, 0), 1)\n",
    "\n",
    "    return bg\n",
    "\n",
    "# rescales the image from (0, 1) to (0, 255)\n",
    "def inv_normalize(im):\n",
    "    assert np.max(im) <= 1 and np.min(im) >= 0, \"Image is not scaled between 0 and 1\"\n",
    "    # im.min() = im.max() float32 (normalization not done channel-wise)\n",
    "\n",
    "    return im * 255\n",
    "\n",
    "def get_loaders(\n",
    "        db_root_dir,\n",
    "        batch_size,\n",
    "        train_transform,\n",
    "        val_transform,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,):\n",
    "    \n",
    "    train_ds = AudDataSet(train=True, db_root_dir=db_root_dir, transform=train_transform,\n",
    "                          pad_mirroring=PAD_MIRRORING)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = AudDataSet(train=False, db_root_dir=db_root_dir, transform=val_transform,\n",
    "                        pad_mirroring=PAD_MIRRORING)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        # I have to shuffle otherwise the saved_images are retrieved from\n",
    "        # the \"bike-packing\" class that is composed by very similar images\n",
    "        # otherwise it would be unnecessary\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# define train_loop\n",
    "def train_loop(model, loader, optim, loss_fn, scaler, pos_weight=False):\n",
    "    loop = tqdm(loader)\n",
    "    loss_20_batches = 0\n",
    "    loss_epoch = 0\n",
    "    for idx, (image, mask) in enumerate(loop):\n",
    "        # transferring data to cpu or gpu\n",
    "        image = image.to(DEVICE)\n",
    "        mask = mask.float().unsqueeze(dim=1).to(DEVICE)\n",
    "\n",
    "        # float16 training: reduces the load to the VRAM and speeds up the training\n",
    "        with torch.amp.autocast():\n",
    "            out = model(image)\n",
    "            if pos_weight:\n",
    "                loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=(mask==0.).sum()/mask.sum())\n",
    "            loss = loss_fn(out, mask)\n",
    "            loss_20_batches += loss\n",
    "            loss_epoch += loss\n",
    "\n",
    "        # backpropagation\n",
    "        # check docs here https://pytorch.org/docs/stable/amp.html\n",
    "        optim.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        if idx%20==0:\n",
    "            loop.set_postfix(loss_20_batches=loss_20_batches.item()/20)\n",
    "            loss_20_batches = 0\n",
    "\n",
    "    print(\n",
    "        f\"==> training_loss: {loss_epoch/len(loader):2f}\"\n",
    "    )\n",
    "\n",
    "def evalution_metrics(model,\n",
    "                      val_loader,\n",
    "                      loss_fn,\n",
    "                      device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    loss_epoch = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, mask) in enumerate(val_loader):\n",
    "            image = image.to(device)\n",
    "            mask = mask.float().unsqueeze(dim=1).to(DEVICE)\n",
    "            pred = model(image)\n",
    "\n",
    "            loss_epoch += loss_fn(pred, mask)\n",
    "            mask_pred = torch.sigmoid(pred)\n",
    "            mask_pred = (mask_pred > 0.5).float()\n",
    "\n",
    "            num_correct += (mask_pred == mask).sum()\n",
    "            num_pixels += torch.numel(mask_pred)\n",
    "            dice_score += (2 * (mask_pred * mask).sum()) / (\n",
    "                    (mask_pred + mask).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {(num_correct/num_pixels)*100:.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"==> valuation_loss: {loss_epoch/len(val_loader):2f}\"\n",
    "    )\n",
    "\n",
    "    print(f\"==> dice_score: {dice_score/len(val_loader)}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "def validation_recall(model,\n",
    "                      val_loader,\n",
    "                      device=DEVICE):\n",
    "    model.eval()\n",
    "    tot_recall = 0\n",
    "    with torch.no_grad():\n",
    "        for image, mask in val_loader:\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device).unsqueeze(dim=1)\n",
    "            out = torch.sigmoid(model(image))\n",
    "            out = np.array((out>0.5).cpu(), dtype=np.uint8).reshape(1,-1).squeeze()\n",
    "            mask = np.array(mask.cpu(), dtype=np.uint8).reshape(1,-1).squeeze()\n",
    "            recall_batch = recall_score(mask, out)\n",
    "            tot_recall += recall_batch\n",
    "\n",
    "    model.train()\n",
    "    print(f'Recall on validation set is: {tot_recall/len(val_loader)}')\n",
    "\n",
    "def save_checkpoint(state, folder_path, filename=\"my_checkpoint.pth.tar\"):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    print(\"=> Saving checkpoint...\")\n",
    "    torch.save(state, os.path.join(folder_path, filename))\n",
    "\n",
    "def load_model_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading model checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def load_optim_checkpoint(checkpoint, optim):\n",
    "    print(\"=> Loading optimizer checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    optim.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "def save_images(model, loader, folder, epoch, device, num_images, pad_mirroring):\n",
    "    print(\"=> Saving images...\")\n",
    "\n",
    "    path = os.path.join(folder, f\"epoch_{epoch + 1}\")\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, masks) in enumerate(loader):\n",
    "            if idx < num_images:\n",
    "                images = images.to(device)\n",
    "                outs = torch.sigmoid(model(images))\n",
    "                outs = (outs > 0.5).float()\n",
    "                if pad_mirroring:\n",
    "                    images = CenterCrop((IMAGE_HEIGHT, IMAGE_WIDTH))(images)\n",
    "                # if the batch_size is > 1 we take just the first image/mask\n",
    "\n",
    "                # plotting the first image/mask per batch\n",
    "                image = images[0]\n",
    "                mask = masks[0]\n",
    "                out = outs[0]\n",
    "\n",
    "                image = tens2image(image)\n",
    "                mask = tens2image(mask)\n",
    "                out = tens2image(out)\n",
    "\n",
    "                img_gt = overlay_mask(inv_normalize(image), mask)\n",
    "                img_out = overlay_mask(inv_normalize(image), out)\n",
    "\n",
    "                fig = plt.figure(figsize=(10, 7))\n",
    "                rows = 1\n",
    "                columns = 2\n",
    "\n",
    "                fig.add_subplot(rows, columns, 1)\n",
    "                plt.imshow(img_gt)\n",
    "                plt.axis('off')\n",
    "                plt.title(\"Input image and ground_truth mask\")\n",
    "\n",
    "                fig.add_subplot(rows, columns, 2)\n",
    "                plt.imshow(img_out)\n",
    "                plt.axis('off')\n",
    "                plt.title(\"Input image and predicted_mask\")\n",
    "\n",
    "                fig.savefig(f'{path}/image_{idx}.png')\n",
    "\n",
    "                plt.cla()\n",
    "                plt.close(fig)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "def predict_image(model, image, val_transform, folder, image_title, pad_mirroring):\n",
    "    path = os.path.join(folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.to(DEVICE)\n",
    "        image = val_transform(image=image)[\"image\"].to(DEVICE).unsqueeze(dim=0)\n",
    "        if pad_mirroring:\n",
    "            image = Pad(padding=pad_mirroring, padding_mode=\"reflect\")(image)\n",
    "        start = time.time()\n",
    "        mask = torch.sigmoid(model(image))\n",
    "        mask = (mask > 0.5).float()\n",
    "        end = time.time()\n",
    "        print(\"Inference time is {:2f}\".format(end-start))\n",
    "\n",
    "\n",
    "        image = tens2image(image)\n",
    "        mask = tens2image(mask)\n",
    "        pred = overlay_mask(inv_normalize(image), mask)\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        plt.imshow(pred)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Test Prediction\")\n",
    "\n",
    "        fig.savefig(f\"{path}/{image_title}.jpg\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81863f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# we are going to train and compare 3 models:\n",
    "# 1) input and target 388x388 --> apply mirroring to input to widen it to 572\n",
    "# 2) input and target 572x572 --> on the mask during forward()\n",
    "# 2) input and target 572x572, model with padding.\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "scaler = torch.amp.GradScaler()\n",
    "model = UNET(3, 64, 1, padding=0, downhill=4).to(DEVICE)\n",
    "optim = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "if CHECKPOINT:\n",
    "    load_model_checkpoint(CHECKPOINT, model)\n",
    "    load_optim_checkpoint(CHECKPOINT, optim)\n",
    "\n",
    "train_loader, val_loader = get_loaders(db_root_dir=ROOT_DIR, batch_size=8, train_transform=train_transform,\n",
    "                                       val_transform=val_transforms, num_workers=4)\n",
    "for epoch in range(17, EPOCHS):\n",
    "\n",
    "    print(f\"Training epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loop(model=model, loader=train_loader, loss_fn=loss_fn, optim=optim, scaler=scaler, pos_weight=False)\n",
    "\n",
    "    print(\"Computing valuation metrics on val_loader...\")\n",
    "\n",
    "    evalution_metrics(model, val_loader, loss_fn, device=DEVICE)\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optim.state_dict(),\n",
    "    }\n",
    "\n",
    "    save_checkpoint(checkpoint, folder_path=SAVE_MODEL_PATH,\n",
    "                    filename=f\"checkpoint_epoch_{epoch+1}.pth.tar\")\n",
    "    save_images(model=model, loader=val_loader, folder=SAVE_IMAGES_PATH,\n",
    "                epoch=epoch, device=DEVICE, num_images=10, pad_mirroring=PAD_MIRRORING)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
